{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(d2l.DataModule):\n",
    "    def __init__(self, num_train, num_val, num_inputs, batch_size):\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X = torch.randn(n, num_inputs)\n",
    "        noise = torch.randn(n,1) * 0.01\n",
    "        w, b = torch.ones((num_inputs, 1)) * 0.01, 0.05\n",
    "        self.y = torch.matmul(self.X, w) + b + noise\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        \"\"\"\n",
    "        这个函数的作用是获取数据加载器。\n",
    "        \n",
    "        它根据是训练集还是验证集来选择相应的数据切片,\n",
    "        然后使用 get_tensorloader 方法创建并返回一个 TensorLoader 对象。\n",
    "        \n",
    "        参数:\n",
    "        train (bool): 如果为 True,返回训练数据加载器;如果为 False,返回验证数据加载器。\n",
    "        \n",
    "        返回:\n",
    "        TensorLoader: 包含选定数据的数据加载器。\n",
    "        \"\"\"\n",
    "        # slice函数用于创建一个切片对象,用来选择数据的子集\n",
    "        # 如果是训练集(train=True),选择从索引0到num_train的数据\n",
    "        # 如果是验证集(train=False),选择从num_train到末尾的数据\n",
    "        # 确保 train 是布尔值\n",
    "    def get_dataloader(self, train):\n",
    "        i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader([self.X, self.y], train, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(w):\n",
    "    return (w ** 2).sum() / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDecayScratch(d2l.LinearRegressionScratch):\n",
    "    def __init__(self, num_inputs, lambd, lr, sigma=0.01):\n",
    "        super().__init__(num_inputs, lr, sigma)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        # 第一项是均方误差项，第二项是权重衰减项\n",
    "        return (super().loss(y_hat, y) +\n",
    "                self.lambd * l2_penalty(self.w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(num_train=20, num_val=100, num_inputs=200, batch_size=5)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "\n",
    "def train_scratch(lambd):\n",
    "    model = WeightDecayScratch(num_inputs=200, lambd=lambd, lr=0.01)\n",
    "    model.board.yscale='log'\n",
    "    trainer.fit(model, data)\n",
    "    print('L2 norm of w:', float(l2_penalty(model.w)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm of w: 0.010814397595822811\n"
     ]
    }
   ],
   "source": [
    "train_scratch(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm of w: 0.0016116609331220388\n"
     ]
    }
   ],
   "source": [
    "train_scratch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDecay(d2l.LinearRegression):\n",
    "    def __init__(self, wd, lr):\n",
    "        super().__init__(lr)\n",
    "        # save_hyperparameters 的作用是将模型的超参数保存下来，以便后续使用和记录。\n",
    "        # 它通常会保存 __init__ 方法中定义的参数，如 lr 和 wd。\n",
    "        # 这对于实验追踪、模型复现和结果分析非常有用。\n",
    "        # 保存的超参数可以在训练过程中被访问，也可以在模型保存时一同保存下来。\n",
    "        self.save_hyperparameters()\n",
    "        self.wd = wd\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        这个函数通常在模型训练开始前被调用，用于配置优化器。\n",
    "        \n",
    "        具体来说，它会在以下情况下被调用：\n",
    "        1. 当使用PyTorch Lightning等高级训练框架时，在训练循环开始前自动调用。\n",
    "        2. 在手动设置训练流程时，通常在创建模型实例后、开始训练循环前显式调用。\n",
    "        3. 如果需要中途更改优化器设置，可能会在训练过程中再次调用。\n",
    "\n",
    "        函数返回配置好的优化器，用于后续的参数更新过程。\n",
    "        \"\"\"\n",
    "        # self.net.weight 是神经网络层的权重参数\n",
    "        # self.net.bias 是神经网络层的偏置参数\n",
    "        # 我们对权重应用权重衰减(weight_decay),但不对偏置应用\n",
    "        return torch.optim.SGD([\n",
    "            {'params': self.net.weight, 'weight_decay': self.wd},\n",
    "            {'params': self.net.bias}\n",
    "        ], lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm of w: 0.014405453577637672\n"
     ]
    }
   ],
   "source": [
    "model = WeightDecay(wd=3, lr=0.01)\n",
    "model.board.yscale = 'log'\n",
    "trainer.fit(model, data)\n",
    "\n",
    "# model.get_w_b() 是一个方法，用于获取模型的权重和偏置参数\n",
    "# 它返回一个元组，其中第一个元素是权重，第二个元素是偏置\n",
    "# 我们使用 [0] 来获取权重部分\n",
    "\n",
    "weight = model.get_w_b()[0]\n",
    "print('权重的L2范数:', float(l2_penalty(weight)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
